We use the same Kaggle notebook GPU environment to train this models.

From the parameter aspect, the VGG16 is the largest one, which contains 50.8M trainable parameters.  The SENet18's total parameters nearly two times of ResNet18. The large amount of parameters may be one reason for the VGG and SENet18 to be overfitted.  Maybe the models are too large but the training set is not large enough. We could use more advanced data augmentation methods like mixup to augment the training data. And tuning the Dropout rate and L2 norm to eliminate the overfitting problem.


In this project, we try to use three CNN models to solve the Cifar100 classification problem. We try our best to tuning the hyper-parameters such as batch size, learning rate, and decay rate etc. Through this project, we have improved our ability to build a CNN model use pytorch architecture and improved the understanding of hyper-parameters. And we are also familiar with the Cifar100 dataset and some image augmentation methods. 

In the future, we will try to use more advanced data augmentation methods like mixup to eliminate the overfitting problem. And we will also try to tunning other hyper parameters such as dropout rate, weight_decay rate, and optimizer methods, etc.

深度学习基础知识+代码
https://zh.d2l.ai/index.html

交叉熵损失函数：https://blog.csdn.net/bitcarmanlee/article/details/105619286
